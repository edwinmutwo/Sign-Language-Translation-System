{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9bbedb7-f817-4fb1-b1ed-a92c4fde40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pyttsx3\n",
    "import joblib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2eebd-d208-4cd4-97ff-4a96915b8475",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e5e7b12-f0e8-4195-ad81-b2957703abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462be278-2c03-4bb7-aa67-b28ac9f22117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f45abb-d088-4229-83ba-a5068ee12a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks_G(image, results):\n",
    "    mp_drawing.draw_landmarks(image,results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(0,255,0), thickness = 2,circle_radius=3),\n",
    "                             mp_drawing.DrawingSpec(color=(0,255,0), thickness = 2,circle_radius=1)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image,results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(0,255,0), thickness = 2,circle_radius=3),\n",
    "                             mp_drawing.DrawingSpec(color=(0,255,0), thickness = 2,circle_radius=1)\n",
    "                             )\n",
    "\n",
    "\n",
    "def draw_styled_landmarks_np_nf_B(image, results):\n",
    "    mp_drawing.draw_landmarks(image,results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(0,0,0), thickness = 2,circle_radius=3),\n",
    "                             mp_drawing.DrawingSpec(color=(0,0,255), thickness = 2,circle_radius=1)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image,results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(0,0,0), thickness = 2,circle_radius=3),\n",
    "                             mp_drawing.DrawingSpec(color=(0,0,255), thickness = 2,circle_radius=1)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b3697-ff3e-41e1-9b38-4dedcf6bed4f",
   "metadata": {},
   "source": [
    "## Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20da807-b757-494d-acf0-063246071697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(text):\n",
    "    engine = pyttsx3.init()\n",
    "    rate = engine.getProperty('rate')\n",
    "    engine.setProperty('rate', 150)\n",
    "\n",
    "    #Setting the voice\n",
    "    voices = engine.getProperty('voices')\n",
    "    engine.setProperty('voice', voices[1].id)\n",
    "\n",
    "    #Text input\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2ee3b-78eb-45e1-afbe-b3352d017ed1",
   "metadata": {},
   "source": [
    "# Make Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc6bad1e-6b1c-46ec-83d4-a3cdaad01715",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_L = joblib.load('MP_model_head.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4306c9ab-b3ba-474a-ac19-4f008daeab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_output(sign_list, sentence, sentence_out):\n",
    "    with open('multi_sign.csv') as multisign_file:\n",
    "        sign_list = csv.reader(multisign_file)\n",
    "        for row in sign_list:\n",
    "            if sentence[-1] == row[-1]:\n",
    "                if sentence[-2] == row[-2]:\n",
    "                    sentence_out.append(row[0])\n",
    "                    break\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7faf98a2-fa73-4494-bbd5-f681d94103e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect(vidsource):\n",
    "    \n",
    "    sentence = []\n",
    "    sentence_out = []\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    last_sign_list = []\n",
    "    one_sign_list = []\n",
    "    \n",
    "    #minimum probability\n",
    "    threshold = 0.9\n",
    "    \n",
    "    #minimum number of predictions for confirmation\n",
    "    pr = 3\n",
    "    \n",
    "    #for fps calculation\n",
    "    pTime = 0\n",
    "    cTime = 0\n",
    "    \n",
    "    #Loading complex signs mechanism\n",
    "    with open('multi_sign.csv') as multisign_file:\n",
    "        sign_list = csv.reader(multisign_file)\n",
    "        for row in sign_list:\n",
    "            last_sign_list.append(row[-1])\n",
    "    \n",
    "    #Loading simple signs\n",
    "    with open('single_sign.csv') as singlesign_file:\n",
    "        singlesign_list = csv.reader(singlesign_file)\n",
    "        for row in singlesign_list:\n",
    "            one_sign_list.append(row[0])\n",
    "    \n",
    "    #Detecting from source of video feed\n",
    "    cap = cv2.VideoCapture(vidsource)\n",
    "    \n",
    "    #set mediapipe model\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        while cap.isOpened():\n",
    "        #for frame_idx in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "\n",
    "            #read frame\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            #make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            \n",
    "            #Draw for tracking\n",
    "            draw_styled_landmarks_np_nf_B(image, results)\n",
    "\n",
    "            #Extract landmarks\n",
    "            lh_row = list(np.array([[landmark.x, landmark.y, landmark.z] for landmark in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3))\n",
    "            rh_row = list(np.array([[landmark.x, landmark.y, landmark.z] for landmark in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3))\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                for id, lm in enumerate(results.pose_landmarks.landmark):\n",
    "                    h,w,c = frame.shape\n",
    "                    cx, cy = int(lm.x*w), int(lm.y*h)\n",
    "         \n",
    "                    if id == 0:\n",
    "                        if lm.visibility > 0.8:\n",
    "                            head = list(np.array([lm.x, lm.y, lm.z]))\n",
    "                        else:\n",
    "                            head =list(np.zeros(1*3))\n",
    "                      \n",
    "            \n",
    "            #Concatenate rows\n",
    "            row = lh_row + rh_row + head\n",
    "\n",
    "            #Make Detections\n",
    "            X = pd.DataFrame([row])\n",
    "            sign_class = model_L.predict(X)[0]\n",
    "            sign_prob = model_L.predict_proba(X)[0]\n",
    "\n",
    "            #Sentence Logic\n",
    "            if sign_prob[np.argmax(sign_prob)] > threshold:\n",
    "                predictions.append(sign_class)\n",
    "\n",
    "                #print(sign_class, sign_prob[np.argmax(sign_prob)])\n",
    "\n",
    "\n",
    "                if predictions[-pr:] == [sign_class]*pr:\n",
    "                    if len(sentence) > 0:\n",
    "                        if sign_class != sentence[-1]:\n",
    "                            sentence.append(sign_class)\n",
    "                            \n",
    "                            #Shows that a detection has been made\n",
    "                            draw_styled_landmarks_G(image, results)\n",
    "                            \n",
    "                            #checks if a sign is a complex sign(last sign of the complex sign)\n",
    "                            if sentence[-1] in last_sign_list:\n",
    "                                sign_output(sign_list, sentence, sentence_out)\n",
    "                            \n",
    "                            #Checks if a sign is a simple sign\n",
    "                            if sentence[-1] in one_sign_list:\n",
    "                                sentence_out.append(sign_class)\n",
    "\n",
    "                            #speak(sign_class)\n",
    "                    else:\n",
    "                        sentence.append(sign_class)\n",
    "                        draw_styled_landmarks_np_nf_B(image, results)\n",
    "                        if sentence[-1] in one_sign_list:\n",
    "                                sentence_out.append(sign_class)\n",
    "                        #speak(sign_class)\n",
    "\n",
    "\n",
    "            if len(sentence) > 5:\n",
    "                    sentence = sentence[-5:]\n",
    "                    \n",
    "                    \n",
    "            if len(sentence_out) > 6:\n",
    "                    sentence_out = sentence_out[-6:]\n",
    "                    #speak(sentence_out)\n",
    "\n",
    "            cv2.rectangle(image, (0,0), (640,40),(0,0,0), -1 )\n",
    "            cv2.putText(image,  ' '.join(sentence), (3,30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            cv2.rectangle(image, (0,80), (640,40),(255,0,0), -1 )\n",
    "            cv2.putText(image,  ' '.join(sentence_out), (3,70),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "            #fps\n",
    "            cTime = time.time()\n",
    "            fps = 1/ (cTime - pTime)\n",
    "            pTime = cTime\n",
    "\n",
    "            cv2.putText(image,\"fps\",(5,415), cv2.FONT_HERSHEY_PLAIN,3,(0,0,0),2)\n",
    "            cv2.putText(image,str(int(fps)),(10,460), cv2.FONT_HERSHEY_PLAIN,3,(0,0,0),5)\n",
    "\n",
    "   \n",
    "            #show to screen\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "            #break loop\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31be636-57fb-439f-970a-2e3fef35f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8bc61-c387-4b0d-8a45-2c48fead1241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad16393c3fd53dbe993a1304d445cc29d6ca41566eeeca851e30b1928e9baf84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
